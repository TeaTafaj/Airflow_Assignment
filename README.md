# TMDB Airflow Pipeline

**Author:** Tea Tafaj  
**Environment:** Apache Airflow 2.10.2 (Python 3.11) + Postgres 13  
**Repository:** https://github.com/TeaTafaj/Airflow_Assignment.git
**Status:** âœ… Successful DAG Run

---

## ğŸ“‹ Table of Contents
1. [Overview](#overview)
2. [Pipeline Workflow](#pipeline-workflow)
3. [Dataset Description](#dataset-description)
4. [Project Structure](#project-structure)
5. [Technical Stack](#technical-stack)
6. [DAG Architecture](#dag-architecture)
7. [Execution Steps](#execution-steps)
8. [Results](#results)
9. [Parallelism & XCom Design](#parallelism--xcom-design)
10. [Screenshots](#screenshots-for-submission)


---

##  Overview
This project builds an **end-to-end data pipeline** using **Apache Airflow** to automate the ingestion, transformation, loading, and analysis of **The Movie Database (TMDB)** datasets. The DAG demonstrates:
- Parallel task execution.
- Proper task dependencies and cleanup.
- Data versioning through layered storage (Raw â†’ Bronze â†’ Silver â†’ Outputs).
- SQL integration via the PostgresOperator.

---

##  Pipeline Workflow
1. **Ingest Data**: Reads two CSVs from `/data/raw/`.
2. **Bronze Stage**: Cleans and validates both datasets (movies and credits).
3. **Silver Stage**: Merges the two cleaned datasets into one enriched dataset.
4. **Load to Postgres**: Creates a schema + table, truncates, then loads new data.
5. **Analysis**: Aggregates top genres by average rating and exports to CSV.
6. **Cleanup**: Deletes intermediate files to maintain a tidy workspace.

---

##  Dataset Description
Source: [Kaggle TMDB 5000 Movie Dataset](https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata)  

| File | Description |
|------|--------------|
| `tmdb_5000_movies.csv` | Core movie metadata including vote counts, genres, popularity, etc. |
| `tmdb_5000_credits.csv` | Cast and crew details, including `movie_id` as key. |

---

##  Project Structure
```
airflow-tmdb/
â”œâ”€ dags/
â”‚  â””â”€ tmdb_pipeline.py
â”œâ”€ include/
â”‚  â””â”€ sql/
â”‚     â””â”€ create_tables.sql
â”œâ”€ data/
â”‚  â”œâ”€ raw/
â”‚  â”‚  â”œâ”€ tmdb_5000_movies.csv
â”‚  â”‚  â””â”€ tmdb_5000_credits.csv
â”‚  â”œâ”€ bronze/
â”‚  â”œâ”€ silver/
â”‚  â””â”€ outputs/
â”‚     â””â”€ top_genres_by_rating.csv
â”œâ”€ .devcontainer/
â”‚  â”œâ”€ docker-compose.yml
â”‚  â”œâ”€ Dockerfile
â”‚  â””â”€ db.env
â””â”€ requirements.txt
```

---

##  Technical Stack
| Component | Purpose |
|------------|----------|
| **Apache Airflow** | Orchestration & task scheduling |
| **PostgreSQL** | Persistent data storage |
| **Docker Compose** | Containerized setup for Airflow + Postgres |
| **Pandas** | Data transformations |
| **Python 3.11** | Core logic and scripting |

---

##  DAG Architecture
```text
create_tables
 â”‚
 â””â”€â”€ ensure_dirs
      â”œâ”€â”€ bronze_movies
      â”œâ”€â”€ bronze_credits
           â”‚
           â””â”€â”€ transform_merge
                â””â”€â”€ truncate_movies
                     â””â”€â”€ load_to_postgres
                          â””â”€â”€ analyze_to_csv
                               â””â”€â”€ cleanup_intermediate
```

- **Parallel branches** for movies and credits ingestion.
- **Transform step** depends on both bronze tasks.
- **Cleanup** runs last to maintain a clean workspace.

---

##  Execution Steps
### Run the pipeline locally
```bash
# 1ï¸âƒ£ Start Airflow and Postgres containers
cd .devcontainer
docker compose up -d

# 2ï¸âƒ£ Access Airflow UI
http://localhost:8080  
Login â†’ Username: admin | Password: admin

# 3ï¸âƒ£ Enable & trigger the DAG
Click toggle â†’ â–¶ï¸ Trigger DAG â†’ Wait for green tasks

# 4ï¸âƒ£ Verify output
cd .devcontainer
docker compose exec airflow bash -lc "ls -la /opt/airflow/data/outputs && head -n 10 /opt/airflow/data/outputs/top_genres_by_rating.csv"
```

---

##  Results
Example output (`data/outputs/top_genres_by_rating.csv`):
```
genre,n_movies,avg_vote
Adventure,356,7.02
Drama,510,6.91
Animation,122,6.84
Action,420,6.73
```

---

##  Parallelism & XCom Design
- **Parallel tasks:** `bronze_movies` and `bronze_credits` run simultaneously to improve runtime.
- **XCom usage:** Only **file paths** (strings) are passed between tasks â€” never raw data.
- **Dependencies:** Downstream tasks wait only on the files they need.

---

##  Screenshots
1. **Airflow DAGs page** â€” showing `tmdb_pipeline` enabled.
![alt text](image.png)
2. **Graph View** â€” all tasks green.
![alt text](image-2.png)


---

**End of README**  
ğŸ©µ _Duke MIDS â€” Data Engineering Assignment, Fall 2025_

